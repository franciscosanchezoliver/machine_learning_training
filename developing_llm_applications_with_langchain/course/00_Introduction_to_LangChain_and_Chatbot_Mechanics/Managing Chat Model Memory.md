
#LangChain #LLM #model_memory

**Managing memory is important for conversations with chat models**, it opens up the possibility of **providing follow-up questions**, of **building and iterating on model responses**, and for chatbots **to adapt to the user's preferences and behaviors**.

Although **LangChain allows us to customize and optimize in-conversation chat memory**, **it is** still **limited by the model's context window**.

An LLM's **context window** is the **amount of input text the model can consider** at once **when generating a response**, and the length of this window varies from different models. 

**LangChain** has a **standard syntax for optimizing model memory**.

We'll cover three LangChain **classes for implementing chatbot memory**. 
- *ChatMessageHistory*
- *ConversationBufferMemory*
- *ConversationSummaryMemory*

## ChatMessageHistory

The *ChatMessageHistory* class **stores the full history of messages** between the user and model.
By providing this to the model, **we can provide follow-up questions and iterate on the response message**. 
Let's implement this message history into an OpenAI model.

```python
from langchain.memory import ChatMessageHistory
from langchain.chat_models import ChatOpenAI

chat = ChatOpenAI(
			temperature=0,
			openai_api_key=openai_api_key
		)

# To begin the conversation history
history = ChatMessageHistory()

# We'll start our conversation with an AI message, which can help
# set the tone and directorion of the conversation.
# We add the message to the history with the "add_ai_message" method
history.add_ai_message("Hi! Ask me anything about LangChain")

# We can add user messages to the history with the "add_user_message"
# method 
history.add_user_message("Describe a metaphor for learning LangChain in one sentence")

# To provide these messages to the model, call the model on the messages
# attribute of the history object.
chat(history.messages)
# => Learning LangChain is like unraveling a complex tapestry of 
# interconnected languages, each thread revealing a new layer of 
# linguistic understanding.
```

When additional user messages are provided, the model bases its response on the full context stored in the conversation history.
```python
history.add_user_message("Summarize the preceding sentence in fewer words")
chat(history.messages)
# => LangChain is a linguistic bridge that connects learners to a world
# of new languages
```

[Example of using Full Message History](03_integrating_a_chatbot_message_history.py)

We can use different tools to manage memory usage in LLM applications. And we can even integrate external data to give the models even more context. 

![Question graph](Ques.canvas)

These different tools can change and process information at different speeds as responses are generated by the system, so it's not the case that one solution fits all cases.

## ConversationBufferMemory

The first **memory optimization tool** we'll cover is *ConversationBufferMemory*. This **gives the application a rolling buffer memory containing the last few messages in the conversation**.

```python
from langchain.memory import ConversationBufferMemory
from langchain_openai import OpenAI
from langchain.chains import ConversationChain

chat = OpenAI(
		model="gpt-3.5-turbo-instruct",
		temperature=0,
		openai_api_key=openai_api_key
)

# We can specify the number of messages stored with the size argument.
# And the applicationw will discard older messages as newer ones are
# added.
memory = ConversationBufferMemory(size=4)

# To integrate the memory type into the model we use a special 
# type of chain for conversations ConversationChain
# (Setting the verbose equal to true allows the model to output 
# its decision along with the results)
buffer_chain = ConversationChain(
					llm=chat,
					memory=memory,
					verbose=True
				)

# Let's pass the chain a series of input.
buffer_chain.predict(input="Describe a language model in one sentence")
buffer_chain.predict(input="Describe it again using less words")
buffer_chain.predict(input="Describe it again fewer words but a lest one word")
buffer_chain.predict(input="What did I first ask you? I forgot.")
```

>Entering a new ConversationChain chain...
> Current Conversation:
> 
> Human: Describe a language model in one sentence
> AI: A language model is a probabilistic model that assigns a probability to a sequence of words in a given language.
> 
> Human: Describe it again using less words
> AI: A language model assigns probabilities to words sequences
> Human: Describe it again fewer words but at least one word
> 
> AI: Probabilistics
> Human: What did I first ask you? I forgot.
> 
> Finished chain.


[Example of using Full Message History](03_integrating_a_chatbot_message_history.py)

## ConversationSummaryMemory

**Summarizing** important points from a conversation **can also be a good way of optimizing memory**.

The *ConversationSummaryMemory* class **summarizes the conversation over time, condensing the information**. This means that **the chat model can remember key pieces of context without needing to store and process the entire conversation history**.

We'll use the *ConversationChain* again, so we'll first instantiate the model to use for the conversation.

```python
from langchain.memory import ConversationSummaryMemory

chat = OpenAI(
		model_name="gtp-3.5-turbo-instruct",
		temperature=0,
		openai_api_key=openai_api_key
		)

memory = ConversationSummaryMemory(
			llm=OpenAI(model_name="gpt-3.5-turbo-instruct),
			openai_api_key=openai_api_key
		)
```

**The *ConversationSummaryMemory*** is different from *ConversationBufferMemory*. It **also takes an LLM as an argument, which is used to create the summaries**.

So what's happening is that, **with each message, a separate LLM call is made to summarize the conversation history**.

Defining the conversation chain is the same as before, highlighting the simplicity and modularity of the LangChain framework.

```python
summary_chain = ConversationChain(
					llm=chat,
					memory=memory,
					verbose=True
)
```

Let's ask the model 3 questions in sequence using *ConversationSummaryMemory*.

```python
summary_chain.predict(input="Please summarize the future in 2 sentences")
summary_chain.predict(input="Why?")
summary_chain.predict(input="What will I need to shape this?")
```

The output shown is the memory that will be used to respond the final question, which we can see is a summary of the first two inputs and responses.


> Entering new ConversationChain chain...
> ...
> Current conversation:
> The human ask the AI to summarize the future in two sentences.
> The AI responds that the future is uncertain and full of posibilities,
> and that is up to us to shape it and make it a better place by taking
> actions and making positive changes. To do this, the AI suggest 
> advocating for policies that promote equity and sustainability, 
> investing in renewable energy sources, and supporting initiatives that
> promote economic growth and opportunity, and to be mindful of the
> impact of decisions and actions on the environment and society. 
> Human: What will I need to shape this?
> AI: 
> Finished chain.



