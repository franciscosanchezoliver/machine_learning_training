# The Evolution of Generative AI


## GANs
We are already familiar with the first model innovation GANs(Generative 
Adversarial Networks).

Remember that these models consists of __generator models and discriminator models
that compete with one another__.

Introduced in 2014, GANs brought a massive leap in the quality of results that
generative techniques could produce.


## Transformer

The second model innovation we'll discuss is the transformer.

This powerful model type is designed to __understand and process text by 
considering multiple words and their relations at once, rather than focusing 
on individual words one at a time__.

Consider the sentence: "The animal didn't cross the street because it was 
too tired". Here the model recognizes that "it" refers to the animal. 

Now, take the sentence: "The animal didn't cross the street because it was 
too wide". In this case, the model understands that "it" refers to the street.

Transformers excel at grasping the context of a given text, which allow them 
to generate more coherent responses.

By analyzing relationships between words and seeing the text as a whole, 
transformers can generate responses that feel natural and informative.


## RLHF (Reinforcement Learning with Human Feedback)

A third important innovation is Reinforcement Learning with Human Feedback. 

This technique __improves models by applying feedback from users__. 

Reinforcement learning teaches models through trial-and-error interactions.

This allows them to learn how to achieve complex and specific goals.

The human feedback part of RLHF comes from users scoring model responses.

This feedback becomes part of a retraining process that helps the model outputs
better match what users score highly.






