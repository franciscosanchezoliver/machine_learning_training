
# Model Evaluation

Let's dive into the last piece of generative AI development: model evaluation.

Evaluation, in the context of generative AI models, is the process of 
assessing the performance and effectiveness of a model based on a set of
defined parameters or tasks.

Quality evaluation serves several key purposes.

First, evaluation measures progress as we train models longer or change 
their design.

Second, evaluation allows model comparison rigorously so we can determine which
model work best for which tasks.


Finally, evaluation can benchmark generative AI against human performance.


